{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray,exp,dot\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softm(S:ndarray)->ndarray:\n",
    "    assert S.ndim == 1\n",
    "                                      # The softmax function.\n",
    "    Z = np.zeros([S.shape[0]])\n",
    "    \n",
    "    k = np.sum(exp(S))\n",
    "    for l in range(len(S)):\n",
    "        Z[l] = (exp(S[l]))/(k)\n",
    "        \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(S:ndarray)->ndarray:   # The sigmoid function.\n",
    "    return (1/(1+np.exp(S)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09003057, 0.24472847, 0.66524096])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Softm(np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The attention Operation with trainable parameters. S is a matrix, each row represents a vector in the sequence.\n",
    "# The attention operation works on a sequence of vectors and outputs a sequence of vectors of the same dimension.\n",
    "\n",
    "def Attention(S:ndarray)->ndarray:                          \n",
    "    \n",
    "    Weights = Weights_attent(S)\n",
    "    \n",
    "    W_q = Weights['W_q']  # The query operation weight tensor. square matrix of order dimension of the vector.\n",
    "    W_k = Weights['W_k']  # The key operation weight tensor.\n",
    "    W_v = Weights['W_v']   # The value operation weight tensor.\n",
    "                                  \n",
    "           # Transforming the input tensor for the respective operations.\n",
    "    \n",
    "    Q = dot(S,W_q.transpose())  # Query \n",
    "    K = dot(S,W_k.transpose()) # The key  ->The input tensor * the transpose of the weight matrix\n",
    "                                                    # to preserve the shape of the input tensor.\n",
    "    V = dot(S,W_v.transpose()) #The value \n",
    "    \n",
    "    # Weight matrix Creation.\n",
    "    \n",
    "    U = dot(Q,K.transpose())\n",
    "    Z = np.zeros([U.shape[0],U.shape[1]])\n",
    "    \n",
    "    for l in range(U.shape[0]):    # Applying the Softmax function row wise.\n",
    "        Z[l,:]  = Softm(U[l,:])\n",
    "        \n",
    "    W = Z/(S.shape[0]**.5)  # Scaling the Weight components.\n",
    "        \n",
    "    Output = dot(W,V)\n",
    "    \n",
    "    return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function creates the matrices for the attention operation.\n",
    "def Weights_attent(S:ndarray)->Dict[str,ndarray]:\n",
    "    dim_v = S.shape[1]  # the dimension of the vectors in the sequence.\n",
    "    \n",
    "    # Weight tensors are weight tensors of shape (dimension of the vectors, dimension of the vectors)\n",
    "    W_q = np.random.randn(dim_v,dim_v)  # Weight matrix for the query operation. The matrix of transformation.\n",
    "    W_k = np.random.randn(dim_v,dim_v) # Weight matrix for the key operation. matrix of transformation.\n",
    "    W_v = np.random.randn(dim_v,dim_v)  # value operation. matrix operation.\n",
    "    \n",
    "    Weights:Dict[str,ndarray] = {'W_q':W_q,'W_k':W_k,'W_v':W_v}\n",
    "        \n",
    "    return Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.random.random((4,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.95543098, -0.43590239, -0.0254599 ],\n",
       "       [ 0.92790575, -0.42375402, -0.02528112],\n",
       "       [ 0.94025067, -0.43104245, -0.02707601],\n",
       "       [ 0.95870903, -0.436121  , -0.02417617]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Operation used With Sigmoid activation.\n",
    "# I tried the attention operation with Sigmoid function.\n",
    "def Attention_1(S:ndarray)->ndarray:\n",
    "    Weights = Weights_attent(S)   # Weight tensors for the attention operation.\n",
    "    \n",
    "    W_q = Weights['W_q']   # Weights for the query operation.\n",
    "    W_k = Weights['W_k']   # Weights for the key operation\n",
    "    W_v = Weights['W_v']   # Weight matrix for the value operation\n",
    "    \n",
    "    # Transforming the input for the three operations.\n",
    "    Q = dot(W_q,S.transpose())  # The query \n",
    "    K = dot(W_k,S.transpose())  # The key \n",
    "    V = dot(W_v,S.transpose())  # The value ->  The 3 linear transformations applied to the sequence of vectors.\n",
    "    \n",
    "    # Creating the weight matrix for the final transformation. \n",
    "    \n",
    "    U = dot(Q,K.transpose())\n",
    "            \n",
    "    Z = np.zeros([U.shape[0],U.shape[1]])\n",
    "    \n",
    "    for n in range(Z.shape[0]):\n",
    "        Z[n,:] = Sigmoid(U[n,:])   # Applying the sigmoid function row wise.\n",
    "        \n",
    "    W = Z/(S.shape[0]**.5)\n",
    "    \n",
    "    Output = dot(W,V)\n",
    "    \n",
    "    return Output\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10197852, -0.14918442, -0.08936732, -0.05045058],\n",
       "       [-0.31142592, -0.58182358, -0.36499999, -0.22930438],\n",
       "       [ 0.12356017,  0.30046345,  0.19568016,  0.13236449]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention_1(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
