{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray,dot\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM layer - The compuational unit of an lstm is a cell(like gru,vanilla versions of rnn's). \n",
    "#The cell is a recursive function which computes the various vectors\n",
    "# associated with the layer, given a timestep. When viewed in this manner, a recurrent layer has feedback connections.\n",
    "\n",
    "# One can unroll the layer, sequence by sequence, then the network can be perceived as a feed forward network.\n",
    "\n",
    "# Any LSTM arcitecture has three gates, controlling the flow data with in the cell.\n",
    "  # 1) Input gate 2) Output gate 3) Forget gate.\n",
    "  # The three gates output - Input gate activation vector, Output gate activation vector and a forget gate activation vector.\n",
    "  # Other vectors associated with the cell are the cell state vector and the hidden/ouput vector.\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM - archtecture 1\n",
    "# The cell \n",
    "# 3 gates - input gate, forget gate and the output gate.\n",
    "# 3 activation vectors associated with three gates.\n",
    "# The cell takes as input three vectors at time t. The input vectors at time t, the hidden vector at time t-1 and the cell\n",
    "# state vector at time t-1.\n",
    "\n",
    "# The cell computes 6 different vectors - the input gate activation vector, the output gate activation vector, the forget\n",
    "# gate activation vector, cell input activation vectors, the cell state vector, and the output/hidden vector.\n",
    "\n",
    "# For computing these vectors, 4 sets of weight tensors are used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions used \n",
    "\n",
    "def tanh(S:ndarray)->ndarray:\n",
    "    return np.tanh(S)\n",
    "\n",
    "def sigmoid(W:ndarray)->ndarray:\n",
    "    return (1/(1+np.exp(-W)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arcitecture 1\n",
    "def LSTM_type_1(S:ndarray,W:Dict[str,ndarray])->ndarray:\n",
    "    assert S.ndim == 2  # A given data point - 2D tensor.\n",
    "    \n",
    "    h_t = np.zeros([W['u_i'].shape[1]])  # initializing hidden vector as zero vector at time t =0.\n",
    "    cell_t = np.zeros([W['u_i'].shape[1]])  # initializing the cell state vector as zero vector at time t =0.\n",
    "    \n",
    "    output_gate_2d = []  # List to store the output gate activation vectors.\n",
    "    hidden_vector_2d = []  # List to store the output/hidden vectors.\n",
    "    input_gate_2d = [] # List to store the input gate activation vectors.\n",
    "    forget_gate_2d = [] # List to store the forget gate activation vectors.\n",
    "    cell_input_2d = [] # List to store the cell input activation vectors\n",
    "    cell_state_2d = [] # List to store the cell state vectors.\n",
    "                                                                                          # Computing\n",
    "    for input_t in S:     # The loop - cell in the lstm layer.\n",
    "        g_t = sigmoid(dot(W['w_g'],input_t) + dot(W['u_g'],h_t) + W['b_g'])  # The forget gate activation vector.\n",
    "        i_t = sigmoid(dot(W['w_i'],input_t) + dot(W['u_i'],h_t) + W['b_i'])   # The input gate activation vector.\n",
    "        o_t = sigmoid(dot(W['w_o'],input_t) + dot(W['u_o'],h_t) + W['b_o'])    # The output gate activation vector.\n",
    "        \n",
    "        c_t = tanh(dot(W['w_c'],input_t) + dot(W['u_c'],h_t) + W['b_c'])  # The cell input activation vector.\n",
    "        \n",
    "        cell_t = g_t*cell_t + i_t*c_t     # The cell state vector.\n",
    "        h_t = o_t*(tanh(cell_t))        # The hidden/output vector.\n",
    "        \n",
    "        output_gate_2d.append(o_t)\n",
    "        hidden_vector_2d.append(h_t)\n",
    "        input_gate_2d.append(i_t)\n",
    "        forget_gate_2d.append(g_t)\n",
    "        cell_input_2d.append(c_t)\n",
    "        cell_state_2d.append(cell_t)\n",
    "        \n",
    "    Outputs:Dict[str,ndarray] = {'Output':hidden_vector_2d,'Output_gate_activations':output_gate_2d,\n",
    "                                'input_gate_activations':input_gate_2d, 'forget_gate_activations':forget_gate_2d,\n",
    "                                'cell_input_activations':cell_input_2d, 'cell_state_vectors':cell_state_2d}\n",
    "        \n",
    "    return Outputs\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.random.random((6,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_i = np.random.randn(4,3)  \n",
    "u_i = np.random.randn(4,4)   # Weight tensors for the input gate\n",
    "b_i = np.random.randn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_g = np.random.randn(4,3)\n",
    "u_g = np.random.randn(4,4)\n",
    "b_g = np.random.randn(4)     # weights for the forget gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_o = np.random.randn(4,3)\n",
    "u_o = np.random.randn(4,4)    # weights for the output gate\n",
    "b_o = np.random.randn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_c = np.random.randn(4,3)\n",
    "u_c = np.random.randn(4,4)   # weights for the cell input activation gate\n",
    "b_c = np.random.randn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "W:Dict['str',ndarray] = {'w_i':w_i,'u_i':u_i,'b_i':b_i,'w_g':w_g,'u_g':u_g,'b_g':b_g,'w_o':w_o,'u_o':u_o,'b_o':b_o,\n",
    "                        'w_c':w_c,'u_c':u_c,'b_c':b_c}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Output': [array([0.17439339, 0.02694052, 0.27625327, 0.20628832]),\n",
       "  array([0.16820564, 0.01714319, 0.46939144, 0.42642058]),\n",
       "  array([0.15065099, 0.02594592, 0.48241038, 0.45394399]),\n",
       "  array([0.20277785, 0.03496466, 0.53784283, 0.53583787]),\n",
       "  array([0.08714051, 0.06191521, 0.54533189, 0.47631811])],\n",
       " 'Output_gate_activations': [array([0.55438195, 0.16819073, 0.57731005, 0.9725676 ]),\n",
       "  array([0.42611739, 0.14280374, 0.63957956, 0.98117514]),\n",
       "  array([0.38611126, 0.15395771, 0.5994467 , 0.98034336]),\n",
       "  array([0.50265993, 0.17054152, 0.59850067, 0.9881205 ]),\n",
       "  array([0.6364184 , 0.23961794, 0.56575661, 0.99133956])],\n",
       " 'input_gate_activations': [array([0.40633609, 0.42544549, 0.93615345, 0.25050981]),\n",
       "  array([0.30470172, 0.51817046, 0.91715033, 0.36917452]),\n",
       "  array([0.29032344, 0.70725064, 0.79037803, 0.58823696]),\n",
       "  array([0.40192294, 0.59791122, 0.92210083, 0.34003067]),\n",
       "  array([0.6750443 , 0.59266961, 0.96540777, 0.14457964])],\n",
       " 'forget_gate_activations': [array([0.40879941, 0.17770204, 0.56288443, 0.62581535]),\n",
       "  array([0.66107515, 0.11254474, 0.47292391, 0.69529723]),\n",
       "  array([0.86578682, 0.08119895, 0.5208258 , 0.73775249]),\n",
       "  array([0.79655632, 0.09491517, 0.61601346, 0.74857532]),\n",
       "  array([0.6944214 , 0.1107132 , 0.7929891 , 0.78490006])],\n",
       " 'cell_input_activations': [array([0.80133708, 0.37976608, 0.55659723, 0.85975276]),\n",
       "  array([0.66343127, 0.19770542, 0.75311513, 0.85543718]),\n",
       "  array([0.17437349, 0.22672947, 0.7893532 , 0.26811692]),\n",
       "  array([0.24763012, 0.32081502, 0.84607826, 0.68290136]),\n",
       "  array([-0.23587408,  0.40724051,  0.86628479,  0.32421538])],\n",
       " 'cell_state_vectors': [array([0.32561218, 0.16156977, 0.52106041, 0.2153765 ]),\n",
       "  array([0.41740277, 0.12062894, 0.93714172, 0.46555629]),\n",
       "  array([0.41200653, 0.17014951, 1.11197501, 0.50118159]),\n",
       "  array([0.42771463, 0.20796867, 1.46516104, 0.60737958]),\n",
       "  array([0.13778874, 0.26438395, 1.99817481, 0.52360721])]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_type_1(S,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture 2 - LSTM\n",
    "\n",
    "# Cell.\n",
    "# 3 gates - output gate, forget gate, input gate.\n",
    "# 2 input vectors into the cell at time t -> the input vector at time t and the cell state vector at time t-1.\n",
    "# 5 output vectors are computed -> output gate activation vector, forget gate activation vector, input gate activation vector,\n",
    "# cell state vector and the hidden/output vector.\n",
    "\n",
    "# 4 set of weights are used for the computations in a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_type_2(S:ndarray,W:Dict[str,ndarray])->Dict[str,ndarray]:\n",
    "    assert S.ndim == 2 # S -2D tensor a datapoint.\n",
    "    h_t = np.zeros([W['u_o'].shape[1]]) # Intializing the hidden vector at time t = 0 as the zero vector.\n",
    "    cell_t = np.zeros([W['u_i'].shape[1]]) # Initializing the cell state vector at time t= 0 as the zero vector.\n",
    "    \n",
    "    input_gate_2d = [] # A list to store the input gate activation vectors.\n",
    "    output_gate_2d = [] # A list to store the output gate activation vectors.\n",
    "    forget_gate_2d = [] # A list to store the forget gate activation vectors computed across the different timesteps.\n",
    "    cell_state_2d = [] # List to store the cell state vectors.\n",
    "    hidden_vector_2d = []  # List to store the hidden/output vectors.\n",
    "    \n",
    "    for input_t in S:  # The layer - applying the cell recursively.\n",
    "        g_t = sigmoid(dot(W['w_g'],input_t) + dot(W['u_g'],cell_t) + W['b_g']) # forget gate activation\n",
    "        o_t = sigmoid(dot(W['w_o'],input_t) + dot(W['u_o'],cell_t) + W['b_o'])  # output gate activation vector\n",
    "        i_t = sigmoid(dot(W['w_i'],input_t) + dot(W['u_i'],cell_t) + W['b_i'])  # input gate activation\n",
    "        cell_t = g_t*cell_t + i_t*tanh(dot(W['w_cell'],input_t) + W['b_cell'])  # cell state vector.\n",
    "        h_t = tanh(o_t*cell_t)   # hidden/output vector\n",
    "        \n",
    "        input_gate_2d.append(i_t)\n",
    "        output_gate_2d.append(o_t)\n",
    "        forget_gate_2d.append(g_t)\n",
    "        cell_state_2d.append(cell_t)\n",
    "        hidden_vector_2d.append(h_t)\n",
    "        \n",
    "    Outputs:Dict[str,ndarray] = {'Output':hidden_vector_2d,'input_gate_activations':input_gate_2d,\n",
    "                                'output_gate_activations':output_gate_2d, 'forget_gate_activations':forget_gate_2d,\n",
    "                                'cell_state_vectors':cell_state_2d}\n",
    "        \n",
    "    return Outputs\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.random.random((5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_i = np.random.randn(4,3)\n",
    "u_i = np.random.randn(4,4)  # The weights for the input gate\n",
    "b_i = np.random.randn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_g = np.random.randn(4,3)\n",
    "u_g = np.random.randn(4,4)  # the weight tensors for the forget gate\n",
    "b_g = np.random.randn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_o = np.random.randn(4,3)\n",
    "u_o = np.random.randn(4,4)  #the weight tensors - output gate\n",
    "b_o = np.random.randn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_cell = np.random.randn(4,3)\n",
    "b_cell = np.random.randn(4)  # the weights - cell state vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "W:Dict[str,ndarray] = {'w_i':w_i,'u_i':u_i,'b_i':b_i,'w_g':w_g,'u_g':u_g,'b_g':b_g,'w_o':w_o,'u_o':u_o,'b_o':b_o,\n",
    "                      'w_cell':w_cell,'b_cell':b_cell}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Output': [array([ 0.043998  , -0.46532108,  0.37986034,  0.17472248]),\n",
       "  array([ 0.13071077, -0.84398672,  0.52900674,  0.15981191]),\n",
       "  array([ 0.27257167, -0.89982642,  0.47380328,  0.0332575 ]),\n",
       "  array([ 0.37563906, -0.96440244,  0.59308615,  0.14129909]),\n",
       "  array([ 0.44684989, -0.99208196,  0.55544949,  0.12635785])],\n",
       " 'input_gate_activations': [array([0.1106751 , 0.59922474, 0.69854514, 0.89807662]),\n",
       "  array([0.17910391, 0.87913883, 0.83274048, 0.83370517]),\n",
       "  array([0.33358193, 0.93778776, 0.68190019, 0.35213479]),\n",
       "  array([0.06162075, 0.842106  , 0.43981007, 0.61605649]),\n",
       "  array([0.02152257, 0.93826267, 0.69557725, 0.71327531])],\n",
       " 'output_gate_activations': [array([0.3982651 , 0.84811662, 0.80517803, 0.59034432]),\n",
       "  array([0.53636741, 0.91657649, 0.56168988, 0.70389607]),\n",
       "  array([0.50934436, 0.96621721, 0.64558847, 0.79744436]),\n",
       "  array([0.65586699, 0.97877357, 0.7865797 , 0.83442332]),\n",
       "  array([0.80773249, 0.97370113, 0.44669217, 0.90007516])],\n",
       " 'forget_gate_activations': [array([0.47761945, 0.78641522, 0.66192464, 0.44547809]),\n",
       "  array([0.59998245, 0.82526116, 0.55748044, 0.19633552]),\n",
       "  array([0.97026999, 0.44203277, 0.94593027, 0.14686757]),\n",
       "  array([0.98485254, 0.79537405, 0.94370096, 0.26693086]),\n",
       "  array([0.95264122, 0.94111672, 0.85184851, 0.10488062])],\n",
       " 'cell_state_vectors': [array([ 0.11054551, -0.59435413,  0.49665591,  0.29903522]),\n",
       "  array([ 0.2450986 , -1.34726569,  1.0482028 ,  0.22900209]),\n",
       "  array([ 0.54901912, -1.52274941,  0.79766485,  0.04172049]),\n",
       "  array([ 0.60221436, -2.04882232,  0.86757063,  0.17047807]),\n",
       "  array([ 0.59519369, -2.8385463 ,  1.40192314,  0.14114027])]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_type_2(S,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
